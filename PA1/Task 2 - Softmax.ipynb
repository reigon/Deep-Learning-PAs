{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1kW0jFRpj3PtumvRbBZe6L2ODb7XLl0Lk","timestamp":1680526450794}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1X4X2hmaEmip"},"source":["# Task 2: Implement Linear Classification with Crossentropy Loss \n","# CENG501 - Spring 2023 - PA1\n","\n","In this task, you will implement a linear classification model with cross-entropy loss. \n","\n","*Disclaimer: Many components in this notebook are adapted or taken from [CS231n](https://cs231n.github.io/) materials.*"]},{"cell_type":"markdown","metadata":{"id":"oRaKrjO5JSu-"},"source":["## 1 Import the Modules\n","\n","Let us start with importing some libraries that we will use throughout the task."]},{"cell_type":"code","metadata":{"id":"v4L5nogMKyNx","executionInfo":{"status":"ok","timestamp":1681330222284,"user_tz":-180,"elapsed":10,"user":{"displayName":"Kaan Akyuz","userId":"14917696553919008338"}}},"source":["import matplotlib.pyplot as plt # For plotting\n","import numpy as np              # NumPy, for working with arrays/tensors \n","import os                       # Built-in library for filesystem access etc.\n","import pickle                   # For (re)storing Python objects into (from) files \n","import time                     # For measuring time\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = [12, 8]\n","plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UMmi17e-JX7o"},"source":["## 2 The Dataset\n","\n","Like Task 1, we will use the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. In this task, we will directly load and continue with preprocessing steps. Please see Task 1 for more details about the dataset. "]},{"cell_type":"markdown","metadata":{"id":"B0mOMB0zOsTp"},"source":["### 2.1 Download the Dataset\n"]},{"cell_type":"code","metadata":{"id":"yBMu6-6mIifW","executionInfo":{"status":"ok","timestamp":1681330235909,"user_tz":-180,"elapsed":13633,"user":{"displayName":"Kaan Akyuz","userId":"14917696553919008338"}},"outputId":"5bce14c1-a071-4fba-f052-4d685a7f50c2","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Download the file\n","! wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","\n","# Extract the compressed file\n","! tar zxvf cifar-10-python.tar.gz"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-04-12 20:10:22--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n","Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 170498071 (163M) [application/x-gzip]\n","Saving to: ‘cifar-10-python.tar.gz’\n","\n","cifar-10-python.tar 100%[===================>] 162.60M  30.1MB/s    in 5.9s    \n","\n","2023-04-12 20:10:28 (27.4 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n","\n","cifar-10-batches-py/\n","cifar-10-batches-py/data_batch_4\n","cifar-10-batches-py/readme.html\n","cifar-10-batches-py/test_batch\n","cifar-10-batches-py/data_batch_3\n","cifar-10-batches-py/batches.meta\n","cifar-10-batches-py/data_batch_2\n","cifar-10-batches-py/data_batch_5\n","cifar-10-batches-py/data_batch_1\n"]}]},{"cell_type":"markdown","metadata":{"id":"zJi0EAJtOvj4"},"source":["### 2.2 Load the Dataset\n","\n","*Disclaimer: This loader is adapted from [CS231n](https://cs231n.github.io/) materials.*"]},{"cell_type":"code","metadata":{"id":"75JGmmE8G90g","executionInfo":{"status":"ok","timestamp":1681330242864,"user_tz":-180,"elapsed":6960,"user":{"displayName":"Kaan Akyuz","userId":"14917696553919008338"}},"outputId":"4ecb620f-d42a-4122-e3fc-f4420a5ba26d","colab":{"base_uri":"https://localhost:8080/"}},"source":["def load_CIFAR_batch(filename):\n","  \"\"\" load single batch of cifar \"\"\"\n","  with open(filename, 'rb') as f:\n","    datadict = pickle.load(f, encoding='latin1')\n","    X = datadict['data']\n","    Y = datadict['labels']\n","    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n","    Y = np.array(Y)\n","    return X, Y\n","\n","def load_CIFAR10(ROOT):\n","  \"\"\" load all of cifar \"\"\"\n","  xs = []\n","  ys = []\n","  for b in range(1,6):\n","    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n","    X, Y = load_CIFAR_batch(f)\n","    xs.append(X)\n","    ys.append(Y)    \n","  Xtr = np.concatenate(xs)\n","  Ytr = np.concatenate(ys)\n","  del X, Y\n","  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n","  return Xtr, Ytr, Xte, Yte\n","\n","def get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000):\n","    \"\"\"\n","    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n","    it for classifiers. These are the same steps as we used for the SVM, but\n","    condensed to a single function.\n","    \"\"\"\n","    # Load the raw CIFAR-10 data\n","    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n","        \n","    # Subsample the data\n","    mask = range(num_training, num_training + num_validation)\n","    X_val = X_train[mask]\n","    y_val = y_train[mask]\n","    mask = range(num_training)\n","    X_train = X_train[mask]\n","    y_train = y_train[mask]\n","    mask = range(num_test)\n","    X_test = X_test[mask]\n","    y_test = y_test[mask]\n","\n","    return X_train, y_train, X_val, y_val, X_test, y_test\n","\n","# Now use these functions to load the dataset:\n","X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data('cifar-10-batches-py/')\n","\n","X_train = np.reshape(X_train, (X_train.shape[0], -1))\n","X_val = np.reshape(X_val, (X_val.shape[0], -1))\n","X_test = np.reshape(X_test, (X_test.shape[0], -1))\n","\n","# Note that 32x32x3 = 3072\n","print('Training data shape: ', X_train.shape)\n","print('Training labels shape: ', y_train.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Training data shape:  (49000, 3072)\n","Training labels shape:  (49000,)\n","Test data shape:  (1000, 3072)\n","Test labels shape:  (1000,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"bZSPB1jyVZ-H"},"source":["### 2.3 Preprocessing\n","\n","We will perform two preprocessing steps: Subtracting the mean and adding a bias dimension to the inputs to make things easier for calculations on the weight matrix.\n","\n","*Disclaimer: This step is taken from [CS231n](https://cs231n.github.io/) materials.*"]},{"cell_type":"code","metadata":{"id":"jmSzBuJeVrVh","executionInfo":{"status":"ok","timestamp":1681330244783,"user_tz":-180,"elapsed":1923,"user":{"displayName":"Kaan Akyuz","userId":"14917696553919008338"}},"outputId":"ac4b4764-b18b-470e-ee5a-6c77ca52c92f","colab":{"base_uri":"https://localhost:8080/","height":369}},"source":["# Preprocessing: subtract the mean image\n","# first: compute the image mean based on the training data\n","mean_image = np.mean(X_train, axis=0)\n","plt.figure(figsize=(4,4))\n","plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean image\n","plt.show()\n","\n","# second: subtract the mean image from train and test data\n","X_train -= mean_image\n","X_test -= mean_image\n","X_val -= mean_image"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 400x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"OzGt3qw0WBc0","executionInfo":{"status":"ok","timestamp":1681330245590,"user_tz":-180,"elapsed":811,"user":{"displayName":"Kaan Akyuz","userId":"14917696553919008338"}},"outputId":"c5d539c0-70e7-43a8-9396-e9def2870961","colab":{"base_uri":"https://localhost:8080/"}},"source":["# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n","# only has to worry about optimizing a single weight matrix W.\n","X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n","X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n","X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n","\n","print('Training data shape: ', X_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Test data shape: ', X_test.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Training data shape:  (49000, 3073)\n","Validation data shape:  (1000, 3073)\n","Test data shape:  (1000, 3073)\n"]}]},{"cell_type":"markdown","metadata":{"id":"fycAQyVdJaHR"},"source":["## 3 Construct the Model\n","\n","In this task, we are defining a model for classification like we did in Task 1. Our model is going to be slightly different this time: Instead of raw scores, our model will output probabilities obtained using a softmax function."]},{"cell_type":"markdown","metadata":{"id":"qm3vEgJ7JgZZ"},"source":["### 3.1 A Model with Probability Outputs\n","\n","In Task 1 and in the lectures, we covered a linear classification model, which performs a simple transformation on input $\\mathbf{x}\\in R^D$ using parameters $W \\in R^{C\\times D}$, where $D$ is the dimensionality of the input and $C$ is the number of classes. This transformation can be formally denoted as (note that $\\mathbf{x}$ contains a constant 1 for the bias):\n","$$\n","\\mathbf{s} = W\\cdot \\mathbf{x}. \n","$$\n","\n","If we look at this operation for class $c$, we see that this is simply a dot product:\n","$$\n","s_c = \\mathbf{w}_c \\cdot \\mathbf{x} = \\sum_{i=1}^D {w}_{ci} x_i. \n","$$\n","\n","We called these $s$ values scores. We can map these scores to probabilities using the softmax function $sm()$:\n","$$\n","p_c = sm(s_c) = \\frac{\\exp(s_c)}{\\sum_j \\exp(s_j)}.\n","$$\n","\n","For numerical stability, subtract the maximum score ($s_{max} = \\max(\\mathbf{s})$):\n","$$\n","p_c = sm(s_c) = \\frac{\\exp(s_c-s_{max})}{\\sum_j \\exp(s_j-s_{max})}.\n","$$\n","\n","\n","Let us first implement this model below. In the following cell, complete the \"@TODO\" part of the `forward()` function. You can directly copy the `update()` and `predict()` functions from Task 1.\n"]},{"cell_type":"code","metadata":{"id":"Ebrij4IDOmLw","executionInfo":{"status":"ok","timestamp":1681330245591,"user_tz":-180,"elapsed":5,"user":{"displayName":"Kaan Akyuz","userId":"14917696553919008338"}}},"source":["class SoftmaxLayer:\n","\n","  def __init__(self, D, C):\n","    \"\"\"\n","      Initialize the softmax layer. Weights are randomly initialized in a small range.\n","\n","      D: The dimensionality of a single instance. \n","      C: The number of classes.\n","    \"\"\"\n","    self.D = D\n","    self.C = C\n","    np.random.seed(501) # for reproducibility, set the seed to a constant\n","    self.W = 0.001 * np.random.randn(D, C)\n","\n","  def forward(self, X, W=None):\n","    \"\"\"\n","    Use the current weights to obtain the probabilities for a batch of samples (X).\n","    You should use a vectorized implementation.\n","\n","    Inputs:\n","    - X: A numpy array of shape (N, D) containing N samples each of dimension D.\n","    \n","    Returns:\n","    \"\"\"\n","    probs = np.zeros((X.shape[0], self.C))\n","    if W is None: W = self.W\n","\n","    ###########################################################################\n","    # @TODO: Implement this method. Store the probs in 'probs' array.         #\n","    # Hint: This should be a single line of code using numpy functions.       #\n","    ###########################################################################\n","    probs = np.exp(X.dot(W)) / np.sum(np.exp(X.dot(W)), axis=1, keepdims=True)\n","    ###########################################################################\n","    #                           END OF YOUR CODE                              #\n","    ###########################################################################\n","    return probs\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Obtain the predictions as discrete labels (integers)\n","\n","    Inputs:\n","    - X: A numpy array of shape (N, D) containing N samples each of dimension D.\n","    Returns:\n","    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n","      array of length N, and each element is an integer giving the predicted\n","      class.\n","    \"\"\"\n","    y_pred = np.zeros(X.shape[0])\n","\n","    ###########################################################################\n","    # @TODO: Copy this part from your implementation of Task 1                #\n","    ###########################################################################\n","    y_pred = np.argmax(self.forward(X, self.W), axis=1)\n","    ###########################################################################\n","    #                           END OF YOUR CODE                              #\n","    ###########################################################################\n","    return y_pred\n","\n","  def update(self, dW, learning_rate):\n","    \"\"\"\n","    Update the model parameters with the gradients (dW) by scaling with the \n","    learning rate.\n","\n","    Inputs:\n","    - dW: The derivative of the loss wrt. W. The shapes of self.W and dW must match.\n","    - learning_rate: The learning rate.\n","    \"\"\"\n","    #########################################################################\n","    # @TODO: Copy this part from your implementation of Task 1.             #\n","    #########################################################################\n","    self.W -= learning_rate*dW\n","    #########################################################################\n","    #                       END OF YOUR CODE                                #\n","    #########################################################################\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1tSQEAw6JloK"},"source":["### 3.2 The Loss Function and the Gradients\n","\n","In this task, we will train a softmax model using cross-entropy loss and L2 regularization on the weights. Formally, we can define this loss as follows:\n","$$\n","L = \\frac{1}{N} \\sum_{i=1}^N - \\log(p_{y_i}) + \\frac{\\lambda}{2}\\sum_{i,j} w_{i,j}^2,\n","$$\n","where $p_{y_i} = sm(s_{y_i})$ is the prediction probability for the correct class for sample $\\mathbf{x}_i$ and $\\lambda$ is the regularization coefficient. As we discussed in the lectures, since we have a single correct class for each input, cross-entropy reduces to the equation above and we also call this negative-log likelihood (NLL) loss.\n","\n","Pay close attention to the loss definition in the above equation. Any discrepancy in your implementation will lead to different outputs. Note that the following implementation includes redundancies in that same quantities are calculated repeatedly. In frameworks, these redundancies are avoided by creating \"computation graphs\" and performing calculations over the computation graph. However, to simplify the implementation and to make the learning parts more explicit, I chose to trade efficiency.\n","\n","In the following code segment, implement the `calculate()` and `gradient()` functions.\n"]},{"cell_type":"code","metadata":{"id":"694vNGbZYmfJ","executionInfo":{"status":"ok","timestamp":1681330245591,"user_tz":-180,"elapsed":4,"user":{"displayName":"Kaan Akyuz","userId":"14917696553919008338"}}},"source":["class NLL_Loss:\n","  def __init__(self, reg):\n","    self.reg = reg\n","  \n","  def calculate(self, W, probs, y):\n","    \"\"\"\n","    NLL loss with L2 regularization. Follow the formulation above closely.\n","\n","    Inputs: \n","    - probs: A numpy array with shape (N, C). The probabilities for each class for each sample.\n","    - y: A numpy array of integers, shape (N). Stores correct labels for samples.\n","\n","    Output: - loss: A floating point number, representing the total loss for the samples.\n","    \"\"\"\n","    loss = 0.0\n","    N = y.size\n","    reg = self.reg\n","\n","    #############################################################################\n","    # @TODO:                                                                    #\n","    # Implement a vectorized version of the NLL loss as described in the above  #\n","    # equation, storing the result in variable `loss`                           #\n","    #############################################################################\n","    loss = np.mean(-np.log(probs[np.arange(N), y])) + reg/2*np.sum(W**2)\n","    #############################################################################\n","    #                             END OF YOUR CODE                              #\n","    #############################################################################\n","    \n","    return loss\n","\n","  def gradient(self, W, X, y, probs):\n","    \"\"\"\n","    Calculates the gradient of the loss wrt. the weights, W. \n","\n","    Inputs:\n","    - X: A numpy array of shape (N, D). The input samples over which we are calculating the gradient.\n","    - y: A numpy array of integers, shape (N). Stores correct labels for samples.\n","    - probs: A numpy array with shape (N, C). The probabilities for each class for each sample.\n","    \n","    Output:\n","    - dW: A numpy array with shape (D, C), same as the shape of W. Stores the gradient\n","    of the loss wrt. W.\n","    \"\"\"\n","    reg = self.reg\n","    dW = np.zeros(W.shape) # initialize the gradient as zero\n","    N = y.size\n","\n","    #############################################################################\n","    # TODO:                                                                     #\n","    # Implement a vectorized version of the gradient for the NLL loss           #\n","    # loss, storing the result in dW.                                           #\n","    #############################################################################\n","    probs[np.arange(N), y] -= 1\n","    dW = 1/N*np.dot(X.T, probs) + reg*W\n","    #############################################################################\n","    #                             END OF YOUR CODE                              #\n","    #############################################################################\n","\n","    return dW\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l9fFCyLojIiZ"},"source":["### 3.3 Check the Gradients\n","\n","Like we did in Task 1, let us check whether your implementation was correct. \n","\n","In the following, you should see differences between the analytical and numerical gradients $\\sim 10^{-8}$. If you see low errors, you can assume that our gradient implementation was correct."]},{"cell_type":"code","metadata":{"id":"cqEVXzt7jI0u","executionInfo":{"status":"ok","timestamp":1681330248050,"user_tz":-180,"elapsed":2462,"user":{"displayName":"Kaan Akyuz","userId":"14917696553919008338"}},"outputId":"d44dd3d3-96c6-4ea5-ec39-21878a67c138","colab":{"base_uri":"https://localhost:8080/"}},"source":["from random import randrange\n","\n","def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n","  \"\"\"\n","  sample a few random elements and only return numerical\n","  in these dimensions.\n","  \"\"\"\n","\n","  for i in range(num_checks):\n","    ix = tuple([randrange(m) for m in x.shape])\n","    oldval = x[ix]\n","\n","    x[ix] = oldval + h # increment a single dimension of x (W) by h\n","    fxph = f(x) # evaluate f(x + h)\n","\n","    x[ix] = oldval - h # decrement by h\n","    fxmh = f(x) # evaluate f(x - h)\n","    \n","    x[ix] = oldval # reset\n","\n","    grad_numerical = (fxph - fxmh) / (2 * h)\n","    grad_analytic = analytic_grad[ix]\n","    rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n","    print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n","\n","def f(W): \n","  probs = model.forward(X_test, W)\n","  return loss_fn.calculate(W, probs, y_test) #W, scores, y):\n","\n","D = X_train.shape[1]\n","C = np.max(y_train)+1\n","model = SoftmaxLayer(D, C)\n","loss_fn = NLL_Loss(reg=1e-5)\n","probs = model.forward(X_test)\n","dW = loss_fn.gradient(model.W, X_test, y_test, probs)\n","grad_numerical = grad_check_sparse(f, x=model.W, analytic_grad=dW, num_checks=10)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["numerical: 0.894612 analytic: 0.894612, relative error: 1.403051e-09\n","numerical: 8.870833 analytic: 8.870833, relative error: 2.489104e-09\n","numerical: -2.257117 analytic: -2.257117, relative error: 4.902461e-09\n","numerical: 1.411033 analytic: 1.411033, relative error: 6.256123e-10\n","numerical: -3.886542 analytic: -3.886542, relative error: 6.611179e-10\n","numerical: -0.124226 analytic: -0.124226, relative error: 1.488850e-07\n","numerical: 0.366500 analytic: 0.366500, relative error: 1.977302e-08\n","numerical: -0.281058 analytic: -0.281058, relative error: 4.670758e-08\n","numerical: -0.126214 analytic: -0.126214, relative error: 6.865090e-08\n","numerical: -2.116096 analytic: -2.116096, relative error: 7.767475e-09\n"]}]},{"cell_type":"markdown","metadata":{"id":"n9pxbgMXJo8k"},"source":["### 3.4 Training Method\n","\n","Okay, it seems that we have most of the ingredients ready, except for a few things. "]},{"cell_type":"code","metadata":{"id":"0Uz286SqU8Ae","executionInfo":{"status":"ok","timestamp":1681330248050,"user_tz":-180,"elapsed":3,"user":{"displayName":"Kaan Akyuz","userId":"14917696553919008338"}}},"source":["def sample_batch(X, y, batch_size):\n","  \"\"\"Get a random batch of size batch_size from (X, y).\"\"\"\n","  batch_indices = np.random.choice(range(X.shape[0]), size=batch_size)\n","  X_batch = X[batch_indices]\n","  y_batch = y[batch_indices]\n","\n","  return X_batch, y_batch\n","\n","def train(model, loss_fn, X, y, learning_rate=1e-3, epochs=10, batch_size=32, verbose=False):\n","  \"\"\"\n","    Train this linear classifier using stochastic gradient descent.\n","    Inputs:\n","    - X: A numpy array of shape (N, D) containing training data; there are N\n","      training samples each of dimension D.\n","    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n","      means that X[i] has label 0 <= c < C for C classes.\n","    - learning_rate: (float) learning rate for optimization.\n","    - reg: (float) regularization strength.\n","    - num_iters: (integer) number of steps to take when optimizing\n","    - batch_size: (integer) number of training examples to use at each step.\n","    - verbose: (boolean) If true, print progress during optimization.\n","    Outputs:\n","    A list containing the value of the loss function at each training iteration.\n","  \"\"\"\n","  num_train, dim = X.shape\n","\n","  # Run stochastic gradient descent to optimize W\n","  loss_history = []\n","  for epoch in range(epochs):\n","    for it in range(int(num_train/batch_size)): \n","        \n","      # Get a batch of samples\n","      X_batch, y_batch = sample_batch(X, y, batch_size)\n","\n","      # Feed-forward through the model\n","      probs = model.forward(X_batch)\n","\n","      # Calculate the loss\n","      loss = loss_fn.calculate(model.W, probs, y_batch)\n","      loss_history.append(loss)\n","\n","      # Calculate the gradient\n","      dW = loss_fn.gradient(model.W, X_batch, y_batch, probs)\n","\n","      # perform parameter update\n","      model.update(dW, learning_rate)\n","\n","    if verbose: print(f'Epoch {epoch} / {epochs}: {loss}')\n","      \n","  return loss_history"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kUmdAA1QJvRx"},"source":["## 4 Train the Model\n","\n","Now, all the pieces are ready and we can train the model. This will be very easy."]},{"cell_type":"code","metadata":{"id":"8Gg_Xp3vrg42"},"source":["num_of_classes = np.max(y_test) + 1 # assume y takes values 0...K-1 where K is number of classes\n","num_of_samples, num_of_dim = X_train.shape\n","\n","model = SoftmaxLayer(num_of_dim, num_of_classes)\n","loss_fn = NLL_Loss(reg=5e4)\n","\n","loss_history = train(model, loss_fn, X_train, y_train, learning_rate=5e-7, epochs=7, batch_size=200, verbose=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"svoFJa6yJysC"},"source":["## 5 Analyze Results\n","\n","We have trained our model and now we should analyze how well it performed. We should look at several factors:\n","\n","* The loss curve.\n","* Quantitative analysis of the performance.\n","* Visual analysis of the weights and the predictions."]},{"cell_type":"markdown","metadata":{"id":"DILBn6y92Ceg"},"source":["### 5.1 The Loss Curve\n","\n","One of the first things we should do when analyzing a model is to plot the loss curve. We should ideally see a smoothly decreasing curve over iterations/epochs."]},{"cell_type":"code","metadata":{"id":"bNZBSrzXiVVe"},"source":["plt.plot(loss_history)\n","plt.xlabel('Iteration number')\n","plt.ylabel('Loss value')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KPMf0jfF2zo6"},"source":["### 5.2 Quantitative Analysis\n","\n","Go back to the `LinearModel` implementation and complete the definition of the `predict()` function. Recreate and retrain the model (as the class definition has been updated) -- i.e. run Section 4 again. Then, we can analyze the accuracy of the predictions as follows. You should see around 32\\%-33\\% accuracies."]},{"cell_type":"code","metadata":{"id":"5PAV1QQw218J"},"source":["y_train_pred = model.predict(X_train)\n","print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n","\n","y_val_pred = model.predict(X_val)\n","print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eopyZ2-F22ce"},"source":["### 5.3 Visual Results\n","\n","Let us look at some visual results. You should see that many of the predictions are off, as we should expect from low accuracy values. This is expected since a linear model is limited for such a classification problem."]},{"cell_type":"code","metadata":{"id":"Nt0hbOSd24hY"},"source":["classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","S = 4 # SxS random samples will be selected and drawn\n","\n","plt.rcParams['figure.figsize'] = [8, 6]\n","\n","for i in range(S):\n","  for j in range(S):\n","    id = np.random.randint(y_test.shape[0])\n","    X = X_test[id]\n","    y = y_test[id]\n","    plt.subplot(S, S, i*S+j+1)\n","    plt.imshow((X[:-1]+mean_image).reshape(32,32,3).astype('uint8'))\n","    pred = model.predict(X.reshape(1, 3073))\n","    plt.axis('off')\n","    plt.title(\"GT: \" + classes[y] + \" Est: \" + classes[pred[0]])\n","    plt.subplots_adjust(hspace = 0.3)\n","\n","plt.show()\n","plt.rcParams['figure.figsize'] = [6, 4]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NarKsB-04VhJ"},"source":["### 5.4 Visualize Weights\n","\n","Let us visualize the weights for each class. You should see images that look like templates for each class."]},{"cell_type":"code","metadata":{"id":"3wpsEnO24YQa"},"source":["w = model.W[:-1,:] # strip out the bias\n","w = w.reshape(32, 32, 3, 10)\n","w_min, w_max = np.min(w), np.max(w)\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","for i in range(10):\n","    plt.subplot(2, 5, i + 1)\n","      \n","    # Rescale the weights to be between 0 and 255\n","    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n","    plt.imshow(wimg.astype('uint8'))\n","    plt.axis('off')\n","    plt.title(classes[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M4AJXtea2427"},"source":["## 6 Tune your Model\n","\n","Like we did in Task 1, tune the hyperparameters for a range of learning rates and regularization strengths. You should be able to obtain ~39\\% accuracies with tuning. You can directly copy-paste this part from Task 1."]},{"cell_type":"code","source":["learning_rates = [1e-7, 1e-6, 1e-5]\n","regularization_strengths = [1e3, 2e3, 3e3, 1e4, 2e4, 3e4, 1e5, 2e3, 3e5]\n","\n","# results is dictionary mapping tuples of the form\n","# (learning_rate, regularization_strength) to tuples of the form\n","# (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n","# of data points that are correctly classified.\n","results = {}\n","best_val = -1   # The highest validation accuracy that we have seen so far.\n","best_svm = None # The LinearLayer object that achieved the highest validation rate.\n","best_lr = None  # The learning rate for the best model\n","best_rg = None  # The regularization strength for the best model\n","\n","for lr in learning_rates:\n","  for rg in regularization_strengths:\n","    ############################################################################\n","    # @TODO: Write your code below in the parts marked with @TODO              #\n","    ############################################################################\n","    \n","    ## @TODO: Create a new SVM instance\n","    model = SoftmaxLayer(D, C)\n","\n","    ## @TODO: Create a new loss instance with current rg and margin=1\n","    loss_fn = NLL_Loss(rg)\n","\n","    ## @TODO: Train with the training set with current lr and rg for 3 epochs\n","    train(model, loss_fn, X_train, y_train, learning_rate=lr, epochs=3, batch_size=32)\n","    \n","    # @TODO: Predict values for training set and the validation set\n","    y_train_pred = model.predict(X_train)\n","    y_val_pred = model.predict(X_val)\n","    \n","    ############################################################################\n","    #                              END OF YOUR CODE                            #\n","    ############################################################################\n","    \n","    # Calculate training and validation accuracies\n","    train_accuracy = np.mean(y_train_pred == y_train)\n","    val_accuracy = np.mean(y_val_pred == y_val)\n","\n","    print(f\"learning rate={lr} and regularization={rg:E} provided train_accuracy={train_accuracy:.3f} and val_accuracy={val_accuracy:.3f}\")\n","    \n","    # Save the results\n","    results[(lr,rg)] = (train_accuracy, val_accuracy)\n","    if best_val < val_accuracy:\n","        best_lr = lr\n","        best_rg = rg\n","        best_val = val_accuracy\n","        best_model = model\n","    \n","print(f'\\nbest validation accuracy achieved during cross-validation: {best_val} with params reg={best_rg} and lr={best_lr}')"],"metadata":{"id":"QO7yFPvYLd4O"},"execution_count":null,"outputs":[]}]}